# MVEL â†’ English Agent (Ollama + LangChain)

An agentic AI system that parses MVEL business rules, explains them in plain English, verifies correctness, generates test cases and self-corrects using reflection 

## What it does
- Parses MVEL `if / else if / else` rules
- Extracts decision branches
- Uses a local LLM via Ollama
- Outputs a human-readable explanation

ğŸ§  Why This Is Agentic AI

This system demonstrates true agentic behavior:

ğŸ§­ Planning â€“ dynamically selects execution steps

ğŸ› ï¸ Tool use â€“ parser, RAG, checker, coverage, etc.

ğŸ” Self-verification â€“ checks its own explanations

ğŸ” Self-correction â€“ rewrites when wrong

ğŸª Reflection â€“ critiques final output

ğŸ§  Memory â€“ persists lessons across runs

ğŸ“Š Observability â€“ full execution traces

This goes far beyond â€œprompt â†’ responseâ€.



agentic_ai/
â”œâ”€â”€ main.py                       # CLI entry point
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ runner.py                 # Orchestrator (agent loop)
â”‚   â”œâ”€â”€ llm.py                    # LLM loader (Ollama)
â”‚   â”œâ”€â”€ memory.py                 # Persistent agent memory
â”‚   â”œâ”€â”€ tracing.py                # Execution tracing
â”‚   â”œâ”€â”€ types.py                  # Agent schemas / dataclasses
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ planner.py            # Planning agent
â”‚   â”‚   â”œâ”€â”€ explainer.py          # Rule explainer
â”‚   â”‚   â”œâ”€â”€ verifier.py           # Explanation verifier
â”‚   â”‚   â”œâ”€â”€ reflect.py            # Reflection / critique agent
â”‚   â”‚   â”œâ”€â”€ tests.py              # Test generation agent
â”‚   â”‚   â””â”€â”€ diff.py               # Rule diff agent
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ mvel_parser_tool.py   # MVEL parser
â”‚       â”œâ”€â”€ static_checker_tool.py# Static rule checks
â”‚       â”œâ”€â”€ rag.py                # RAG retrieval logic
â”‚       â””â”€â”€ dir/                  # ğŸ“š Knowledge base for RAG
â”‚           â””â”€â”€ rules.md
â”œâ”€â”€ runs/                         # Execution trace outputs
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ rule.mvel                 # Sample rule
â””â”€â”€ README.md




## Run
```bash
python main.py examples/sample.mvel


python main.py --mode explain examples/rule.mvel
Verify explanation fidelity
python main.py --mode verify examples/rule.mvel
Generate test cases
python main.py --mode tests examples/rule.mvel
Diff two rules
python main.py --mode diff old.mvel new.mvel